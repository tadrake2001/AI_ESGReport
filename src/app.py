import tiktoken
import streamlit as st
import requests
import pandas as pd
import time
import tempfile
import fitz
import re
import json

# B·ªè ch·ªçn batch size, m·∫∑c ƒë·ªãnh l√† 10
batch_size = 10
MAX_TOKENS_LIMIT = 4096
enc = tiktoken.encoding_for_model("gpt-3.5-turbo")

LM_STUDIO_URL = "http://localhost:1234/v1/chat/completions"

EXCLUDE_PATTERNS = [
    r'(?i)(email|mailto|contact|phone|website|www\.|@)',
    r'\bpage\b\s*\d+',
    r'(?i)(copyright|disclaimer)'
]

HEADER_PATTERNS = [
    r'^\s*\d+(\.\d+)*\s*$',
    r'^\s*(t·ªïng quan|gi·ªõi thi·ªáu|m·ª•c ti√™u|ph·∫°m vi|cam k·∫øt|ch√≠nh s√°ch).*$',
    r'^\s*[A-Z ]{3,}\s*$'
]

def clean_text(text):
    lines = text.splitlines()
    clean_lines = []
    for line in lines:
        if any(re.search(pat, line) for pat in EXCLUDE_PATTERNS):
            continue
        if len(line.strip()) > 0:
            clean_lines.append(line.strip())
    return " ".join(clean_lines)

def is_header_like(text):
    for pattern in HEADER_PATTERNS:
        if re.match(pattern, text.strip(), flags=re.IGNORECASE):
            return True
    return False

def estimate_tokens(text):
    return len(enc.encode(text))

def batch_sentences(sentences, batch_size=10):
    return [sentences[i:i+batch_size] for i in range(0, len(sentences), batch_size)]

def format_input_for_model(batch):
    return {
        "instruction": "H√£y x√°c ƒë·ªãnh c√°c c√¢u c√≥ d·∫•u hi·ªáu ph√≥ng ƒë·∫°i trong b√°o c√°o ESG v√† gi·∫£i th√≠ch l√Ω do.",
        "input": batch
    }

def query_batch(batch):
    formatted_input = format_input_for_model(batch)
    headers = {"Content-Type": "application/json"}
    prompt = json.dumps(formatted_input, ensure_ascii=False, indent=2)
    payload = {
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 1024,
        "stream": False
    }
    try:
        response = requests.post(LM_STUDIO_URL, json=payload, headers=headers)
        response.raise_for_status()
        result = response.json()['choices'][0]['message']['content']
    except Exception as e:
        return [("", {"label": "l·ªói", "reason": str(e)})]

    results = []
    pattern = r"^- (.+?)\n\s*‚Üí\s*(.+)"
    matches = re.findall(pattern, result, re.MULTILINE)
    for sentence, reason in matches:
        results.append((sentence.strip(), {"label": "ph√≥ng ƒë·∫°i", "reason": reason.strip()}))
    return results

# Giao di·ªán
st.set_page_config(page_title="Ph√¢n t√≠ch ESG", layout="wide")
st.title("\U0001F4C4 Ph√¢n t√≠ch B√°o c√°o ESG v√† T√¨m c√¢u ph√≥ng ƒë·∫°i")

uploaded_file = st.file_uploader("\U0001F4C4 T·∫£i b√°o c√°o ESG d·∫°ng PDF", type=["pdf"])
if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name
    st.success("‚úÖ ƒê√£ t·∫£i l√™n th√†nh c√¥ng!")

    with fitz.open(tmp_path) as doc:
        all_text = "\n".join(clean_text(page.get_text()) for page in doc)

    sentences = re.split(r'(?<=[.!?])\s+', all_text)
    filtered_sentences = [s.strip() for s in sentences if 30 < len(s.strip()) < 400 and not is_header_like(s)]
    st.session_state["edited_sentences"] = filtered_sentences
    st.write(f"üîé ƒê√£ ph√°t hi·ªán {len(filtered_sentences)} c√¢u h·ª£p l·ªá t·ª´ b√°o c√°o.")

if "edited_sentences" in st.session_state:
    st.subheader("‚úçÔ∏è Ch·ªânh s·ª≠a c√°c nh√≥m c√¢u tr∆∞·ªõc khi ph√¢n t√≠ch")
    batches = batch_sentences(st.session_state["edited_sentences"], batch_size=batch_size)
    updated_batches = []
    batch_included = []

    for i, batch in enumerate(batches):
        include_group = st.checkbox(f"‚úÖ G·ª≠i nh√≥m {i+1}", value=True, key=f"include_batch_{i}")
        batch_included.append(include_group)

        with st.expander(f"üì¶ Nh√≥m {i+1}"):
            updated = []
            for j, sentence in enumerate(batch):
                edited = st.text_area(f"C√¢u {j+1}:", sentence, key=f"group_{i}_sent_{j}")
                if edited.strip():
                    updated.append(edited.strip())
            updated_batches.append(updated)

    if st.button("\U0001F50D G·ª≠i v√† l·ªçc c√°c c√¢u ph√≥ng ƒë·∫°i"):
        st.info("ƒêang ph√¢n t√≠ch v√† l·ªçc c√°c c√¢u ph√≥ng ƒë·∫°i theo nh√≥m...")
        progress_bar = st.progress(0)
        exaggerated = []
        start_time = time.time()

        for i, (batch, included) in enumerate(zip(updated_batches, batch_included)):
            if not included:
                continue

            formatted_input = format_input_for_model(batch)
            input_text = json.dumps(formatted_input, ensure_ascii=False, indent=2)
            token_count = estimate_tokens(input_text)
            with st.expander(f"\U0001F4E4 Input nh√≥m {i+1} ({token_count} tokens)"):
                st.code(input_text, language="json")
                if token_count > MAX_TOKENS_LIMIT:
                    st.warning(f"‚ö†Ô∏è Nh√≥m n√†y v∆∞·ª£t qu√° gi·ªõi h·∫°n {MAX_TOKENS_LIMIT} tokens, c√≥ th·ªÉ g√¢y l·ªói ho·∫∑c b·ªã c·∫Øt n·ªôi dung.")

            result = query_batch(batch)
            for sentence, res in result:
                if res['label'] == "ph√≥ng ƒë·∫°i":
                    exaggerated.append({"C√¢u": sentence, "L√Ω do": res['reason']})
            progress_bar.progress((i + 1) / len(updated_batches))

        elapsed = time.time() - start_time
        avg_per_batch = elapsed / len(updated_batches) if updated_batches else 0
        st.success(f"‚úÖ Ho√†n t·∫•t trong {elapsed:.1f} gi√¢y (trung b√¨nh {avg_per_batch:.1f} gi√¢y m·ªói nh√≥m)")

        if exaggerated:
            st.subheader(f"‚ö†Ô∏è Ph√°t hi·ªán {len(exaggerated)} c√¢u c√≥ d·∫•u hi·ªáu ph√≥ng ƒë·∫°i")

            st.markdown("""
                    #### üìå Danh s√°ch c√¢u c√≥ d·∫•u hi·ªáu ph√≥ng ƒë·∫°i
                    """)
            for item in exaggerated:
                st.markdown(
                    f"""
                            <div style='background-color:#fff9f5;padding:0.75em 1em;margin-bottom:1em;border-left:6px solid #f06a36;border-radius:6px'>
                                <div style='color:#222;font-weight:600;margin-bottom:0.25em'>üìù {item['C√¢u']}</div>
                                <div style='color:#555'>‚û°Ô∏è <i>{item['L√Ω do']}</i></div>
                            </div>
                            """,
                    unsafe_allow_html=True
                )
            summary = "\n".join(
                f"- {item['C√¢u']}\n  ‚Üí {item['L√Ω do']}"
                for item in exaggerated
            )
            df = pd.DataFrame(exaggerated)
            st.download_button(
                label="\U0001F4E5 T·∫£i danh s√°ch c√¢u ph√≥ng ƒë·∫°i",
                data=df.to_csv(index=False).encode('utf-8'),
                file_name="esg_phong_dai.csv",
                mime="text/csv"
            )
        else:
            st.success("‚úÖ Kh√¥ng c√≥ c√¢u ph√≥ng ƒë·∫°i n√†o ƒë∆∞·ª£c ph√°t hi·ªán.")